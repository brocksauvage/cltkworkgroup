{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Setup\n",
    "\n",
    "This will be a very introductory session to help us all get started on the same page. Today, we will download and install Python, the Classical Language Toolkit (CLTK), the Natural Language Toolkit (NLTK), and a few helper packages that will be beneficial later on! These steps are only necessary if you are using your own computer for this workgroup. The second half of this first session will provide an exercise to demonstrate a perhaps impractical application of computational text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So let's get started: Anaconda, I choose you!\n",
    "\n",
    "Please install the newest version of Anaconda (https://www.anaconda.com/download). This software let's us use Jupyter notebooks (what you're reading from right now!) which is a great way to test code in a modular format, allowing for speedy changes and immensely less frustration! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next up: Installing the CLTK & NLTK\n",
    "\n",
    "Let's install these awesome packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line tells the computer to install the correct packages\n",
    "import sys\n",
    "!{sys.executable} -m pip install cltk\n",
    "\n",
    "# Depending on your machine, you may need to go to the terminal itself and install the CLTK manually.\n",
    "# Come chat with me if this happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the same for the NLTK\n",
    "!{sys.executable} -m pip install -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we just need to install some dependencies\n",
    "\n",
    "The following packages will help us out later on as we start doing more advanced things!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy provides some of the more advanced mathematical things we might need\n",
    "!{sys.executable} -m pip install numpy\n",
    "\n",
    "# pandas is good for data visualization and analysis. We'll do more with these later.\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Latin Palindromes\n",
    "\n",
    "To whet our appetite for *computational text analysis*, let's play around with finding latin palindromes. This exercise comes from Patrick Burns' blog *Disiecta Membra*. \n",
    "Link here: https://disiectamembra.wordpress.com/2017/03/26/finding-palindromes-in-the-latin-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "# These statements set up tools that help us normalize the texts.\n",
    "# They will be discussed in more detail in the next session.\n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "replacer = JVReplacer()\n",
    "\n",
    "# This function uses the previously defined tools to preprocess the texts.\n",
    "# This comes directly w/o modification from Patrick Burns\n",
    "def preprocess(text):    \n",
    "\n",
    "    # Normalizing ligatures\n",
    "    text = re.sub(r'&aelig;','ae',text)\n",
    "    text = re.sub(r'&AElig;','AE',text)\n",
    "    text = re.sub(r'&oelig;','oe',text)\n",
    "    text = re.sub(r'&OElig;','OE',text)\n",
    "    \n",
    "    text = re.sub('\\x00',' ',text)\n",
    "    \n",
    "    #Lowercasing all the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    #Replacing j's & v's\n",
    "    text = replacer.replace(text)\n",
    "    \n",
    "    # More normalizing work\n",
    "    text= re.sub(r'&lt;','<',text)\n",
    "    text= re.sub(r'&gt;','>',text)    \n",
    "    \n",
    "    # Getting rid of punctuation\n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    # Getting rid of some standard Latin Library titles\n",
    "    remove_list = [r'\\bthe latin library\\b',\n",
    "                   r'\\bthe classics page\\b',\n",
    "                   r'\\bneo-latin\\b', \n",
    "                   r'\\bmedieval latin\\b',\n",
    "                   r'\\bchristian latin\\b',\n",
    "                   r'\\bthe miscellany\\b'\n",
    "                  ]\n",
    "\n",
    "    for pattern in remove_list:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\s+\\n+\\s+','\\n', text) # Remove double lines and trim spaces around new lines\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "corpus_importer = CorpusImporter('latin')\n",
    "corpus_importer.list_corpora\n",
    "\n",
    "corpus_importer.import_corpus('latin_models_cltk')\n",
    "corpus_importer.import_corpus('latin_text_latin_library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Latin Library corpus\n",
    "\n",
    "from cltk.corpus.readers import get_corpus_reader\n",
    "ll = get_corpus_reader(language='latin', \n",
    "                       corpus_name='latin_text_latin_library')\n",
    "files = ll.fileids()\n",
    "print(files[:50]) # The first 50 files in the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats\n",
    "\n",
    "file_count = len(files)\n",
    "print(f'There are {file_count} files in this corpus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the raw text of the entire Latin Library\n",
    "latinlibrary_whole = ll.raw()\n",
    "print(latinlibrary_whole[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use our handy-dandy function from P.B. to process the raw text.\n",
    "ll_text = preprocess(latinlibrary_whole)\n",
    "print(ll_text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line splits the text based on whitespace. We don't need a fancy method\n",
    "# for splitting enclitics or anything here, since we are only interested in \n",
    "# whether a word, even with an enclitic, forms a palindrome.\n",
    "ll_tokens = ll_text.split()\n",
    "\n",
    "# We remove all tokens(words) that are shorter than 3 characters.\n",
    "ll_tokens = [token for token in ll_tokens if len(token) > 2]\n",
    "\n",
    "# We remove tokens made up of a single character.\n",
    "ll_tokens = [token for token in ll_tokens if token != len(token)*token[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a function to check if a word is a palindrome or not:\n",
    "def is_palindrome(token):\n",
    "    return token == token[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we should filter out all the tokens(words) from the Latin Library\n",
    "# for palindromes. This line will make a list of all the palindromes in \n",
    "# this corpus.\n",
    "palindromes = [token for token in ll_tokens if is_palindrome(token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many are there total?\n",
    "print(len(palindromes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can determine the most common ones:\n",
    "c = Counter(palindromes)\n",
    "print(c.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can make a list of the longest palindromes\n",
    "palindromes = [k for k, c in c.items()]\n",
    "palindromes.sort(key=len, reverse=True)\n",
    "\n",
    "# This line let's us see how many unique palindromes exist in this corpus\n",
    "print(len(palindromes))\n",
    "print(palindromes[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
