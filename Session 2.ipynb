{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Getting Familiar with the CLTK, Lemmatization, and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we will explore the basic functionality and syntax of the Classical Language Toolkit (CLTK). The CLTK extends the Natural Language Toolkit (NLTK), the foremost tool for Natural Language Processing (NLP) in most languages, and better fits it to the specific needs of ancient languages. There are many facets to this Python package, so we will only be familiarizing ourselves with a few of the most common functionalities--essentially the bread and butter of computational text analysis with the CLTK.\n",
    "\n",
    "We'll cover the following:\n",
    "1. Downloading & Importing the CLTK\n",
    "2. Importing Greek & Latin Texts\n",
    "3. Tokenization\n",
    "4. Lemmatization\n",
    "\n",
    "So let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to import the CLTK package before anything else\n",
    "import cltk\n",
    "\n",
    "# Specifically from CLTK, we need the Corpus Importer which will allow us to download texts\n",
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "# The way the Corpus Importer has been created, we must create a python *object* that will\n",
    "# serve as the downloader, as it were, of our texts\n",
    "latinDL = CorpusImporter('latin')\n",
    "\n",
    "# The downloader object has an *attribute* that allows to see what corpora are available\n",
    "latinDL.list_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we know what corpora are available, we can select one to download\n",
    "# For this session, we need 'latin_text_latin_library' & 'latin_models_cltk'\n",
    "\n",
    "# This line will automatically download the entirety of the Latin Library to your\n",
    "# computer in the form of text files that are easy for the machine to read.\n",
    "latinDL.import_corpus('latin_text_latin_library')\n",
    "\n",
    "# This line downloads a helper corpus of linguistic models which allow us to \n",
    "# lemmatize the text and do a whole bunch of other neat things.\n",
    "latinDL.import_corpus('latin_models_cltk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Read-In Texts\n",
    "\n",
    "In this module, we will read in the text files for Vergil's *Aeneid*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our texts and models downloaded, we need to import them.\n",
    "# We need another package called 'os' that allows Python to make use of the operating system.\n",
    "import os\n",
    "\n",
    "# You will have to navigate to the location on your machine where the texts are downloaded\n",
    "# to find the filenames. All of the data that CLTK uses on your machine will be in a folder\n",
    "# entitled 'cltk_data' in your home directory. This is the same directory that holds folders\n",
    "# such as 'Documents' and 'Desktop'. We need to summon this textual data.\n",
    "\n",
    "# Some of the texts are split-up by book, like Vergil's Aeneid, so we need a good way of\n",
    "# gathering all that information into the system. Below, I have created a list of the filenames\n",
    "# without the '.txt' appended.\n",
    "filenames = ['aen1','aen2','aen3','aen4','aen5','aen6','aen7','aen8','aen9','aen10','aen11','aen12']\n",
    "\n",
    "# Let's create a list called 'aeneid' that will hold all the text. It will have 12 entries, one\n",
    "# for each book/file. We can leave it blank for now because we're about to read in all the files.\n",
    "aeneid=[]\n",
    "\n",
    "# Now we will use a for loop to iterate over all the filenames we just created.\n",
    "# This basically says: for each file in filenames, open the file and read in it's context to the\n",
    "# proper place in the list 'aeneid'. \n",
    "for filename in filenames:\n",
    "    with open(f'/Users/chaduhl/cltk_data/latin/text/latin_text_latin_library/vergil/{filename}.txt') as f:\n",
    "        aeneid.append(f.read())\n",
    "        \n",
    "# Let's make sure we did that right. The line below should print out the first book of the\n",
    "# Aeneid. Remember that counting in python starts at 0.\n",
    "print(aeneid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we move any further, we need to account for small inconsistencies, such as\n",
    "# j/i and u/v transcriptions, especially since the Latin Library is fond of j's and\n",
    "# both using both u's and v's. \n",
    "\n",
    "# We need to import a special package from CLTK to take care of this.\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "j = JVReplacer()\n",
    "for i in range(12):\n",
    "    aeneid[i] = j.replace(aeneid[i].lower())\n",
    "print(aeneid[0])\n",
    "\n",
    "# And we need to make everything lowercase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Tokenize Texts\n",
    "\n",
    "Having successfully read-in the text, we now need to tokenize it. Tokenizing a text means splitting up continuous text into chunks--usually either chunks of sentences or individual words. For the purposes of this exercise, we will be chunking the text into work-tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tokenize a text is to split up all the words into individual entities. Luckily, the\n",
    "# CLTK has a built-in function for doing this. First, we need to import the tokenizer package:\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "\n",
    "# Then, just like the Corpus Importer, we need to create an object called 'tokenizer' that\n",
    "# will do the job. We also need to tell the tokenizer what language it will be tokenizing.\n",
    "tokenizer = WordTokenizer('latin')\n",
    "\n",
    "# Let's create a list to hold all these tokens\n",
    "aeneid_tokens = []\n",
    "\n",
    "# A for-loop to actually implement the tokenizer\n",
    "for i in range(12):\n",
    "    aeneid_tokens.append(tokenizer.tokenize(aeneid[i]))\n",
    "\n",
    "# Let's see if it worked. You should see the first 100 \"words\" of Book 1 of the Aeneid. \n",
    "# There will be numbers and punctuation intermingled because we have not yet removed them.\n",
    "# In the line below, there are two sets of brackets because we have created a 2-dimensional\n",
    "# array: a list of lists. The first set of brackets indicates the book and the second indicates\n",
    "# the which token or word we are calling. In this case, the line calls the first 50 tokens.\n",
    "print(aeneid_tokens[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean out those line numbers and punctuation with for-loop and a simple regular expression:\n",
    "for i in range(12):\n",
    "    aeneid_tokens[i] = [token for token in aeneid_tokens[i] if any(c.isalpha() for c in token)]\n",
    "\n",
    "print(aeneid_tokens[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Lemmatize Texts\n",
    "\n",
    "Now that we have the text split into word-tokens, we can lemmatize it. Lemmatizing entails stripping each word of its inflection or declension. We want each word to be in the form of its most basic dictionary entry. This should not be done in all cases, but it is important for many methods of computational text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to import the Lemmatizer package & create a Lemmatizer object.\n",
    "# The syntax here should be starting to look familiar.\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "\n",
    "aeneid_lemmata = []\n",
    "for i in range(12):\n",
    "    aeneid_lemmata.append(lemmatizer.lemmatize(aeneid_tokens[i]))\n",
    "print(aeneid_lemmata[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
